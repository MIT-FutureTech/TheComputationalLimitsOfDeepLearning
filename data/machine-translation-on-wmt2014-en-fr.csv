name,year,parameters,title,paper_with_code,paper_link,flops,multiadds,fp_bp_ratio,training_epochs,training_set_size,training_dataset,training_time_total,training_time_total_sec,training_time_tpu_core_days,training_gpu_model,training_gpu_amount,training_gpu_ptp_gflops,training_tpu_model,training_tpu_amount,training_tpu_ptp_gflops,training_cpu_model,training_cpu_amount,training_cpu_ptp_gflops,pretraining_epochs,pretraining_set_size,pretraining_dataset,pretraining_time_total,pretraining_time_total_sec,pretraining_time_tpu_core_days,pretraining_gpu_model,pretraining_gpu_amount,pretraining_gpu_ptp_gflops,pretraining_tpu_model,pretraining_tpu_amount,pretraining_tpu_ptp_gflops,pretraining_cpu_model,pretraining_cpu_amount,pretraining_cpu_ptp_gflops,finetuning_epochs,finetuning_set_size,finetuning_dataset,finetuning_time_total,finetuning_time_total_sec,finetuning_time_tpu_core_days,finetuning_gpu_model,finetuning_gpu_amount,finetuning_gpu_ptp_gflops,finetuning_tpu_model,finetuning_tpu_amount,finetuning_tpu_ptp_gflops,finetuning_cpu_model,finetuning_cpu_amount,finetuning_cpu_ptp_gflops,network_operations,hardware_burden,comments,(BLEU),(SACREBLEU)
name,year,parameters,paper,,,ops_forward_pass,,fp_bp_ratio,training,,,,,,,,,,,,,,,pre_training,,,,,,,,,,,,,,,fine_tuning,,,,,,,,,,,,,,,net_ops,hw_burden,comments,performance_metrics,
,,,title,pwc_url,paper_url,flops,multiadds,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,,,,bleu,sacrebleu
,,,,,,,,,,,,total,total_sec, tpu_core_days,model,amount,ptp_gflops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,total,total_sec, tpu_core_days,model,amount,ptp_gflops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,total,total_sec, tpu_core_days,model,amount,ptp_flops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,,
Transformer+BT (ADMIN init),2020,,Very Deep Transformers for Neural Machine Translation,/paper/very-deep-transformers-for-neural-machine,https://arxiv.org/abs/2008.07772v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,46.40,44.4
Noisy back-translation,2018,,Understanding Back-Translation at Scale,/paper/understanding-back-translation-at-scale,https://www.aclweb.org/anthology/D18-1045/,,,3,,,,27hrs 40mins,9.96E+04,,Tesla V100 PCIe 32 GB,128,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.80E+20,,45.60,43.8
mRASP+Fine-Tune,2020,,Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,/paper/pre-training-multilingual-neural-machine,https://www.aclweb.org/anthology/2020.emnlp-main.210,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,44.30,41.7
Transformer + R-Drop,2021,,R-Drop: Regularized Dropout for Neural Networks,/paper/r-drop-regularized-dropout-for-neural,https://arxiv.org/abs/2106.14448v1,,,3,,,,500 min,3.00E+04,,GeForce RTX 3090,8,3.56E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.54E+18,,43.95,
Admin,2020,,Understanding the Difficulty of Training Transformers,/paper/understanding-the-difficulty-of-training,https://www.aclweb.org/anthology/2020.emnlp-main.463,,,3,,,,2.5 days,2.16E+05,,Tesla V100 PCIe 32 GB,16,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.88E+19,,43.80,
BERT-fused NMT,2020,,Incorporating BERT into Neural Machine Translation,/paper/incorporating-bert-into-neural-machine-1,https://openreview.net/forum?id=Hyl7ygStwB,,,3,,,,10 days,8.64E+05,,Tesla M40,8,6.84E+03,,,,,,,,,,14 days,1.21E+06,,Tesla M40,8,6.84E+03,,,,,,,,,,,,,,,,,,,,,,,1.14E+20,,43.78,
MUSE(Paralllel Multi-scale Attention),2019,,MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning,/paper/muse-parallel-multi-scale-attention-for,https://arxiv.org/abs/1911.09483v1,,,3,,,,,,,GeForce RTX 2080 Ti,4,1.34E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,43.50,
T5,2019,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning,https://arxiv.org/abs/1910.10683v3,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,43.40,
Local Joint Self-attention,2019,,Joint Source-Target Self Attention with Locality Constraints,/paper/190506596,https://arxiv.org/abs/1905.06596v1,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,43.30,
Depth Growing,2019,,Depth Growing for Neural Machine Translation,/paper/depth-growing-for-neural-machine-translation,https://www.aclweb.org/anthology/P19-1558/,,,3,,,,6 days,5.18E+05,,Tesla M40,8,6.84E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.84E+19,,43.27,
Transformer Big,2018,,Scaling Neural Machine Translation,/paper/scaling-neural-machine-translation,https://www.aclweb.org/anthology/W18-6301/,,,3,,,,8.5 hrs,3.06E+04,,Tesla V100 PCIe 32 GB,128,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.53E+19,,43.20,
TaLK Convolutions,2020,,Time-aware Large Kernel Convolutions,/paper/time-aware-large-kernel-convolutions,https://proceedings.icml.cc/static/paper_files/icml/2020/5762-Paper.pdf,,,3,,,,,,,GeForce RTX 2080 Ti,8,1.34E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,43.20,
LightConv,2019,,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://openreview.net/forum?id=SkVhlh09tX,,,3,,,,,,,Tesla P100 PCIe 16 GB,1,9.53E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,43.10,
FLOATER-large,2020,,Learning to Encode Position for Transformer with Continuous Dynamical Model,/paper/learning-to-encode-position-for-transformer,https://proceedings.icml.cc/static/paper_files/icml/2020/955-Paper.pdf,,,3,,,,,,,Tesla V100 PCIe 32 GB,2,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,42.70,
OmniNetP,2021,,OmniNet: Omnidirectional Representations from Transformers,/paper/omninet-omnidirectional-representations-from,https://arxiv.org/abs/2103.01075v1,,,3,,,,,,,,,,Cloud TPU v3,2,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,42.60,
T2R + Pretrain,2021,,Finetuning Pretrained Transformers into RNNs,/paper/finetuning-pretrained-transformers-into-rnns,https://arxiv.org/abs/2103.13076v1,,,3,,,,82hrs,2.95E+05,,Tesla V100 PCIe 32 GB,1,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.17E+18,,42.10,
Transformer Big + MoS,2018,,Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation,/paper/fast-and-simple-mixture-of-softmaxes-with-bpe,https://arxiv.org/abs/1809.09296v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,42.10,
Synthesizer (Random + Vanilla),2020,,Synthesizer: Rethinking Self-Attention in Transformer Models,/paper/synthesizer-rethinking-self-attention-in,https://arxiv.org/abs/2005.00743v3,,,3,,,,,,,,,,Cloud TPU v3,1,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41.85,
Transformer (big),2018,,Self-Attention with Relative Position Representations,/paper/self-attention-with-relative-position,https://www.aclweb.org/anthology/N18-2074/,,,3,,,,,,,Tesla P100 PCIe 16 GB,8,9.53E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41.50,
ConvS2S (ensemble),2017,,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://icml.cc/Conferences/2017/Schedule?showEvent=806,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41.44,
Weighted Transformer (large),2017,,Weighted Transformer Network for Machine Translation,/paper/weighted-transformer-network-for-machine,https://openreview.net/forum?id=SkYMnLxRW,,,3,,,,,,,Tesla K80,,5.60E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41.40,
Evolved Transformer Big,2019,2.21E+08,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/abs/1901.11117v4,,,3,,,,,,,,,,Cloud TPU v2,4,1.80E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41.30,
RNMT+,2018,3.79E+08,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,/paper/the-best-of-both-worlds-combining-recent,https://www.aclweb.org/anthology/P18-1008/,2.81E+10,,3,,,,120h,4.32E+05,,Tesla P100 PCIe 16 GB,32,9.53E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.32E+20,,41.00,
Transformer (big),2017,,Attention Is All You Need,/paper/attention-is-all-you-need,http://papers.nips.cc/paper/7181-attention-is-all-you-need,,,3,,,,3.5days,3.02E+05,,Tesla P100 PCIe 16 GB,8,9.53E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.30E+19,,41.00,
ResMLP-12,2021,6.38E+07,ResMLP: Feedforward networks for image classification with data-efficient training,/paper/resmlp-feedforward-networks-for-image,https://arxiv.org/abs/2105.03404v2,,,3,,,,2 days,1.73E+05,,Tesla V100 PCIe 32 GB,8,1.41E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.95E+19,,40.60,
MoE,2017,8.70E+09,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/abs/1701.06538v1,,,3,,,,6 days,5.18E+05,,Tesla K40,64,4.29E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.42E+20,,40.56,
Transformer,2019,,Memory-Efficient Adaptive Optimization,/paper/memory-efficient-adaptive-optimization-for,https://arxiv.org/pdf/1901.11150v2.pdf,,,3,,,,,,,,,,Cloud TPU v2,2,1.80E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.50,
Deep-Att + Ensemble + PosUnk,2016,,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://www.aclweb.org/anthology/Q16-1027/,,,3,,,,10 days,8.64E+05,,Tesla K40,24,4.29E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.90E+19,,40.40,
TransformerBase + AutoDropout,2021,,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,/paper/autodropout-learning-dropout-patterns-to,https://arxiv.org/abs/2101.01761v1,,,3,,,,,,,,,,Cloud TPU v2,1,1.80E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40.00,
GNMT+RL,2016,,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,/paper/googles-neural-machine-translation-system,https://arxiv.org/abs/1609.08144v2,,,3,,,,6 days,5.18E+05,,Tesla K80,96,5.60E+03,,,,,,,,,,3 days,2.59E+05,,Tesla K80,96,5.60E+03,,,,,,,,,,,,,,,,,,,,,,,4.18E+20,,39.92,
Rfa-Gate-arccos,2021,,Random Feature Attention,/paper/random-feature-attention-1,https://openreview.net/forum?id=QtTKTdVrFBB,,,3,,,,,,,,,,Cloud TPU v3,16,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,39.20,
LSTM6 + PosUnk,2014,,Addressing the Rare Word Problem in Neural Machine Translation,/paper/addressing-the-rare-word-problem-in-neural,https://www.aclweb.org/anthology/P15-1002/,,,3,,,,14 days,1.21E+06,,,8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,37.50,
SMT+LSTM5,2014,,Sequence to Sequence Learning with Neural Networks,/paper/sequence-to-sequence-learning-with-neural,https://arxiv.org/pdf/1409.3215v3.pdf,,,3,,,,10 days,8.64E+05,,,8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36.50,
RNN-search50*,2014,,Neural Machine Translation by Jointly Learning to Align and Translate,/paper/neural-machine-translation-by-jointly,https://arxiv.org/pdf/1409.0473v7.pdf,,,3,,,,5 days,4.32E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36.15,
Deep Convolutional Encoder; single-layer decoder,2016,,A Convolutional Encoder Model for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://arxiv.org/pdf/1606.04199v3.pdf,,,3,,,,,,,Tesla M40,1,6.84E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35.70,
CSLM + RNN + WP,2014,,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,/paper/learning-phrase-representations-using-rnn,https://arxiv.org/pdf/1406.1078v3.pdf,,,3,,,,3 days,2.59E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.54,
FLAN 137B zero-shot,2021,,Finetuned Language Models Are Zero-Shot Learners,/paper/finetuned-language-models-are-zero-shot,https://arxiv.org/pdf/2109.01652v3.pdf,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.00,
Regularized LSTM,2014,,Recurrent Neural Network Regularization,/paper/recurrent-neural-network-regularization,https://arxiv.org/pdf/1409.2329v5.pdf,,,3,,,,,,,Tesla K20,1,3.52E+03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.03,
Unsupervised PBSMT,2018,,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/abs/1804.07755v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.11,
GRU+Attention,2016,,Can Active Memory Replace Attention?,/paper/can-active-memory-replace-attention,https://arxiv.org/pdf/1610.08613v2.pdf,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26.40,
SMT + iterative backtranslation (unsupervised),2018,,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://arxiv.org/pdf/1809.01272v1.pdf,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26.22,
Semi-supervised (full) + 100k parallel,2018,,Unsupervised Neural Machine Translation,/paper/unsupervised-neural-machine-translation,https://arxiv.org/pdf/1710.11041v2.pdf,,,3,,,,5 days,4.32E+05,,GeForce Titan X,1,1.10E+04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.74E+18,,21.81,