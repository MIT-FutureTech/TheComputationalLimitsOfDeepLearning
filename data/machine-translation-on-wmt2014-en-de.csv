name,year,parameters,title,paper_with_code,paper_link,flops,multiadds,fp_bp_ratio,training_epochs,training_set_size,training_dataset,training_time_total,training_time_total_sec,training_time_tpu_core_days,training_gpu_model,training_gpu_amount,training_gpu_ptp_gflops,training_tpu_model,training_tpu_amount,training_tpu_ptp_gflops,training_cpu_model,training_cpu_amount,training_cpu_ptp_gflops,pretraining_epochs,pretraining_set_size,pretraining_dataset,pretraining_time_total,pretraining_time_total_sec,pretraining_time_tpu_core_days,pretraining_gpu_model,pretraining_gpu_amount,pretraining_gpu_ptp_gflops,pretraining_tpu_model,pretraining_tpu_amount,pretraining_tpu_ptp_gflops,pretraining_cpu_model,pretraining_cpu_amount,pretraining_cpu_ptp_gflops,finetuning_epochs,finetuning_set_size,finetuning_dataset,finetuning_time_total,finetuning_time_total_sec,finetuning_time_tpu_core_days,finetuning_gpu_model,finetuning_gpu_amount,finetuning_gpu_ptp_gflops,finetuning_tpu_model,finetuning_tpu_amount,finetuning_tpu_ptp_gflops,finetuning_cpu_model,finetuning_cpu_amount,finetuning_cpu_ptp_gflops,network_operations,hardware_burden,comments,(BLEU),(SACREBLEU)
name,year,parameters,paper,,,ops_forward_pass,,fp_bp_ratio,training,,,,,,,,,,,,,,,pre_training,,,,,,,,,,,,,,,fine_tuning,,,,,,,,,,,,,,,net_ops,hw_burden,comments,performance_metrics,
,,,title,pwc_url,paper_url,flops,multiadds,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,epochs,set_size,dataset_name,time,,,gpu,,,tpu,,,cpu,,,,,,bleu,sacrebleu
,,,,,,,,,,,,total,total_sec, tpu_core_days,model,amount,ptp_gflops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,total,total_sec, tpu_core_days,model,amount,ptp_gflops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,total,total_sec, tpu_core_days,model,amount,ptp_flops,model,amount,ptp_gflops,model,amount,ptp_gflops,,,,,
Transformer Cycle (Rev),2021,,Lessons on Parameter Sharing across Layers in Transformers,/paper/lessons-on-parameter-sharing-across-layers-in,https://arxiv.org/abs/2104.06022v1,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35.14,33.54
Noisy back-translation,2018,,Understanding Back-Translation at Scale,/paper/understanding-back-translation-at-scale,https://www.aclweb.org/anthology/D18-1045/,,,3,,,,22.5 hrs,8.10E+04,,Tesla V100 PCIe 32 GB,128,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.46E+20,,35.00,33.8
Transformer+Rep (Uni),2021,,Rethinking Perturbations in Encoder-Decoders for Fast Training,/paper/rethinking-perturbations-in-encoder-decoders,https://www.aclweb.org/anthology/2021.naacl-main.460,,,3,,,,,,,Tesla V100 PCIe 32 GB,8,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33.89,32.35
T5-11B,2019,1.10E+10,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,/paper/exploring-the-limits-of-transfer-learning,https://arxiv.org/abs/1910.10683v3,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32.10,
BiBERT,2021,,"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation",/paper/bert-mbert-or-bibert-a-study-on,https://arxiv.org/pdf/2109.04588v1.pdf,,,3,,,,4 weeks,2.42E+06,,,,,Cloud TPU v3,1,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.02E+21,,31.26,
Transformer + R-Drop,2021,,R-Drop: Regularized Dropout for Neural Networks,/paper/r-drop-regularized-dropout-for-neural,https://arxiv.org/abs/2106.14448v1,,,3,,,,,,,Tesla V100 PCIe 32 GB,8,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.91,
BERT-fused NMT,2020,,Incorporating BERT into Neural Machine Translation,/paper/incorporating-bert-into-neural-machine-1,https://openreview.net/forum?id=Hyl7ygStwB,,,3,,,,24 days,2.07E+06,,Tesla M40,8,6844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.14E+20,,30.75,
Data Diversification - Transformer,2019,,Data Diversification: A Simple Strategy For Neural Machine Translation,/paper/data-diversification-an-elegant-strategy-for,http://proceedings.neurips.cc/paper/2020/hash/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Abstract.html,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.70,
Mask Attention Network (big),2021,2.15E+08,Mask Attention Networks: Rethinking and Strengthen Transformer,/paper/mask-attention-networks-rethinking-and,https://www.aclweb.org/anthology/2021.naacl-main.135,,,3,,,,,,,Tesla P40,4,11760,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.40,
Transformer (ADMIN init),2020,,Very Deep Transformers for Neural Machine Translation,/paper/very-deep-transformers-for-neural-machine,https://arxiv.org/abs/2008.07772v2,,,3,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30.10,29.5
Depth Growing,2019,,Depth Growing for Neural Machine Translation,/paper/depth-growing-for-neural-machine-translation,https://www.aclweb.org/anthology/P19-1558/,,,3,,,,6 days,1.04E+06,,Tesla M40,8,6844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.68E+19,,29.92,
MUSE (Parallel Multi-scale Attention),2019,,MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning,/paper/muse-parallel-multi-scale-attention-for,https://arxiv.org/abs/1911.09483v1,,,3,,,,,,,GeForce RTX 2080 Ti,4,13447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.90,
OmniNetP,2021,,OmniNet: Omnidirectional Representations from Transformers,/paper/omninet-omnidirectional-representations-from,https://arxiv.org/abs/2103.01075v1,,,3,,,,,,,,,,Cloud TPU v3,8,32,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.80,
Evolved Transformer Big,2019,,The Evolved Transformer,/paper/the-evolved-transformer,https://arxiv.org/abs/1901.11117v4,2.18E+08,,3,,,,,,,Tesla P100 PCIe 16 GB,8,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.80,29.2
Local Joint Self-attention,2019,,Joint Source-Target Self Attention with Locality Constraints,/paper/190506596,https://arxiv.org/abs/1905.06596v1,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.70,
DynamicConv,2019,2.13E+08,Pay Less Attention with Lightweight and Dynamic Convolutions,/paper/pay-less-attention-with-lightweight-and,https://openreview.net/forum?id=SkVhlh09tX,,,3,,,,,,,Tesla V100 PCIe 32 GB,8,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.70,
Transformer Big + MoS,2018,,Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation,/paper/fast-and-simple-mixture-of-softmaxes-with-bpe,https://arxiv.org/abs/1809.09296v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.60,
TaLK Convolutions,2020,,Time-aware Large Kernel Convolutions,/paper/time-aware-large-kernel-convolutions,https://arxiv.org/pdf/2002.03184.pdf,,,3,,,,,,,GeForce RTX 2080 Ti,8,13447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.60,
Transformer Big + adversarial MLE,2019,,Improving Neural Language Modeling via Adversarial Training,/paper/improving-neural-language-modeling-via,https://arxiv.org/abs/1906.03805v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.52,
Transformer Big,2018,,Scaling Neural Machine Translation,/paper/scaling-neural-machine-translation,https://www.aclweb.org/anthology/W18-6301/,,,3,,,,85 min,5.10E+03,,Tesla V100 PCIe 32 GB,128,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.22E+18,,29.30,
Subformer-xlarge,2021,,Subformer: A Parameter Reduced Transformer,/paper/subformer-a-parameter-reduced-transformer,https://openreview.net/forum?id=6UurSaf08jx,,,3,,,,,,,Tesla V100 PCIe 32 GB,16,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.30,
SB-NMT,2019,,Synchronous Bidirectional Neural Machine Translation,/paper/synchronous-bidirectional-neural-machine,https://www.aclweb.org/anthology/Q19-1006/,,,3,,,,,,,GeForce Titan Xp,3,11366.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.21,
FLOATER-large,2020,,Learning to Encode Position for Transformer with Continuous Dynamical Model,/paper/learning-to-encode-position-for-transformer,https://proceedings.icml.cc/static/paper_files/icml/2020/955-Paper.pdf,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.20,
Local Transformer,2018,,Modeling Localness for Self-Attention Networks,/paper/modeling-localness-for-self-attention,https://www.aclweb.org/anthology/D18-1475/,,,3,,,,,,,Tesla P40,8,11760,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.20,
Transformer (big),2018,,Self-Attention with Relative Position Representations,/paper/self-attention-with-relative-position,https://www.aclweb.org/anthology/N18-2074/,,,3,,,,,,,Tesla P100 PCIe 16 GB,8,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.20,
Transformer Big with FRAGE,2018,,FRAGE: Frequency-Agnostic Word Representation,/paper/frage-frequency-agnostic-word-representation,http://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29.11,
adequacy-oriented NMT,2018,,Neural Machine Translation with Adequacy-Oriented Learning,/paper/neural-machine-translation-with-adequacy,https://arxiv.org/abs/1811.08541v1,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.99,
universal transformer base,2018,,Universal Transformers,/paper/universal-transformers,https://openreview.net/forum?id=HyzdRiR9Y7,,,3,,,,,,,Tesla P100 PCIe 16 GB,8,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.90,
Weighted Transformer (large),2017,2.13E+08,Weighted Transformer Network for Machine Translation,/paper/weighted-transformer-network-for-machine,https://openreview.net/forum?id=SkYMnLxRW,,,3,,,,,,,Tesla K80,,5600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.90,
T2R + Pretrain,2021,,Finetuning Pretrained Transformers into RNNs,/paper/finetuning-pretrained-transformers-into-rnns,https://arxiv.org/abs/2103.13076v1,,,3,,,,82 hrs,2.95E+05,,Tesla V100 PCIe 32 GB,1,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.17E+18,,28.70,
KERMIT,2019,,KERMIT: Generative Insertion-Based Modeling for Sequences,/paper/kermit-generative-insertion-based-modeling,https://arxiv.org/abs/1906.01604v1,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.70,
RNMT+,2018,3.79E+08,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,/paper/the-best-of-both-worlds-combining-recent,https://www.aclweb.org/anthology/P18-1008/,2.81E+10,,3,24.6,,,40 hrs,1.44E+05,,Tesla P100 PCIe 16 GB,32,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.39E+19,,28.49,
Synthesizer (Random + Vanilla),2020,,Synthesizer: Rethinking Self-Attention in Transformer Models,/paper/synthesizer-rethinking-self-attention-in,https://arxiv.org/abs/2005.00743v3,,,3,,,,,,,,,,Cloud TPU v3,1,8,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.47,
Transformer Big,2017,,Attention Is All You Need,/paper/attention-is-all-you-need,http://papers.nips.cc/paper/7181-attention-is-all-you-need,,,3,40,,,3.5 days,3.02E+05,,Tesla P100 PCIe 16 GB,8,9526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.30E+19,,28.40,
Transformer + SRU,2017,,Simple Recurrent Units for Highly Parallelizable Recurrence,/paper/simple-recurrent-units-for-highly,https://www.aclweb.org/anthology/D18-1477/,,,3,,,,,,,Tesla V100 PCIe 32 GB,8,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.40,
Rfa-Gate-arccos,2021,,Random Feature Attention,/paper/random-feature-attention-1,https://openreview.net/forum?id=QtTKTdVrFBB,,,3,,,,,,,,,,Cloud TPU v3,16,128,4.20E+05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.20,
Transformer-DRILL Base,2019,,Deep Residual Output Layers for Neural Language Generation,/paper/deep-residual-output-layers-for-neural,https://arxiv.org/abs/1905.05513v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28.10,
CMLM+LAT+4 iterations,2020,,Incorporating a Local Translation Mechanism into Non-autoregressive Translation,/paper/incorporating-a-local-translation-mechanism,https://www.aclweb.org/anthology/2020.emnlp-main.79,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27.35,
ResMLP-12,2021,,ResMLP: Feedforward networks for image classification with data-efficient training,/paper/resmlp-feedforward-networks-for-image,https://arxiv.org/abs/2105.03404v2,,,3,,,,2 days,1.73E+05,,Tesla V100 PCIe 32 GB,8,14130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.95E+19,,26.80,
ConvS2S (ensemble),2017,,Convolutional Sequence to Sequence Learning,/paper/convolutional-sequence-to-sequence-learning,https://icml.cc/Conferences/2017/Schedule?showEvent=806,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26.43,
GNMT+RL,2016,,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,/paper/googles-neural-machine-translation-system,https://arxiv.org/abs/1609.08144v2,,,3,,,,,,,Tesla K80,,5600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26.30,
SliceNet,2017,,Depthwise Separable Convolutions for Neural Machine Translation,/paper/depthwise-separable-convolutions-for-neural,https://openreview.net/forum?id=S1jBcueAb,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26.10,
MoE,2017,8.70E+09,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,/paper/outrageously-large-neural-networks-the,https://arxiv.org/abs/1701.06538v1,,,3,,,,1 day,8.64E+04,,Tesla K40,64,4291.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.37E+19,,26.03,
DenseNMT,2018,,Dense Information Flow for Neural Machine Translation,/paper/dense-information-flow-for-neural-machine,https://www.aclweb.org/anthology/N18-1117/,,,3,,,,,,,Tesla M40,1,6844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25.52,
ByteNet,2016,,Neural Machine Translation in Linear Time,/paper/neural-machine-translation-in-linear-time,https://arxiv.org/abs/1610.10099v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23.75,
FlowSeq-large,2019,,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,/paper/flowseq-non-autoregressive-conditional,https://www.aclweb.org/anthology/D19-1437/,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23.72,
Ensemble 8 models + unk replace ,2015,,Effective Approaches to Attention-based Neural Machine Translation,/paper/effective-approaches-to-attention-based,https://www.aclweb.org/anthology/D15-1166/,,,3,,,,8.5 days,7.34E+05,,Tesla K40,1,4291.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.15E+18,,23.00,
Denoising autoencoders (non-autoregressive),2018,,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,/paper/deterministic-non-autoregressive-neural,https://www.aclweb.org/anthology/D18-1149/,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21.54,
Phrase Based MT,2015,,Edinburgh's Syntax-Based Systems at WMT 2015,/paper/edinburghs-syntax-based-systems-at-wmt-2015,https://www.aclweb.org/anthology/W15-3024/,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.70,
Deep-Att,2016,,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,/paper/deep-recurrent-models-with-fast-forward,https://www.aclweb.org/anthology/Q16-1027/,,,3,,4.50E+06,,10 days,8.64E+05,,Tesla K40,24,4291.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.90E+19,,20.60,
PBSMT + NMT,2018,,Phrase-Based & Neural Unsupervised Machine Translation,/paper/phrase-based-neural-unsupervised-machine,https://arxiv.org/abs/1804.07755v2,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.23,
NAT +FT + NPD,2017,,Non-Autoregressive Neural Machine Translation,/paper/non-autoregressive-neural-machine-translation-1,https://openreview.net/forum?id=B1l8BtlCb,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19.17,
Seq-KD + Seq-Inter + Word-KD,2016,8.00E+06,Sequence-Level Knowledge Distillation,/paper/sequence-level-knowledge-distillation,https://www.aclweb.org/anthology/D16-1139/,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18.50,
NSE-NSE,2016,,Neural Semantic Encoders,/paper/neural-semantic-encoders,https://www.aclweb.org/anthology/E17-1038/,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.90,
SMT + iterative backtranslation (unsupervised),2018,,Unsupervised Statistical Machine Translation,/paper/unsupervised-statistical-machine-translation,https://www.aclweb.org/anthology/D18-1399/,,,3,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.08,